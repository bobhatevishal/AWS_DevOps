
# 1) Quick roadmap (order to follow)

1. Prep your environment (AWS account, CLI, tools, repo)
2. Create basic infra (S3 bucket, DynamoDB table, SNS topic) — dev manually or with SAM
3. Create IAM role & policy for Lambda
4. Write Lambda processor (Rekognition → DynamoDB → SNS)
5. Deploy Lambda and wire S3 event → Lambda
6. Test with sample images, tune thresholds
7. Add presigned-upload endpoint (optional) + simple frontend uploader
8. Add search API (Lambda + API Gateway) or OpenSearch if scale
9. CI/CD (GitHub Actions) + infra-as-code (SAM / CloudFormation / CDK)
10. Monitoring, alarms, cost controls, security hardening, docs & README

---

# 2) Prep (Do this first)

* Create an AWS account / use dev account.
* Create an IAM user for deployment (programmatic access). For quick dev, attach `AdministratorAccess` — but for production use least-privilege.
* Install locally:

  * AWS CLI v2 — `aws configure` to set credentials.
  * Python 3.11, pip
  * AWS SAM CLI (optional but recommended) + Docker (for `sam local`)
  * git

Commands:

```bash
# configure AWS CLI
aws configure
# install python packages inside venv
python3 -m venv venv && source venv/bin/activate
pip install boto3
```

---

# 3) Create initial infra (manual CLI short steps)

You can do these via AWS Console too. Use unique bucket name.

Create S3 bucket (example):

```bash
BUCKET_NAME="myapp-original-images-$(date +%s)"
aws s3api create-bucket --bucket $BUCKET_NAME --region us-east-1
# block public access from console or add bucket-policy later
```

Create DynamoDB table (simple schema, on-demand):

```bash
aws dynamodb create-table \
  --table-name Images \
  --attribute-definitions AttributeName=image_id,AttributeType=S \
  --key-schema AttributeName=image_id,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST
```

Create SNS topic:

```bash
TOPIC_ARN=$(aws sns create-topic --name image-moderation-alerts --query 'TopicArn' --output text)
echo $TOPIC_ARN
# subscribe admin email (will require confirmation)
aws sns subscribe --topic-arn $TOPIC_ARN --protocol email --notification-endpoint you@example.com
```

---

# 4) Create IAM role for Lambda (minimum needed)

Create trust policy (`trust.json`):

```json
{
  "Version":"2012-10-17",
  "Statement":[
    {
      "Effect":"Allow",
      "Principal":{"Service":"lambda.amazonaws.com"},
      "Action":"sts:AssumeRole"
    }
  ]
}
```

Create role:

```bash
aws iam create-role --role-name lambda-image-processor-role --assume-role-policy-document file://trust.json
```

Attach inline policy (example `policy.json` — adapt ARNs / REGION / ACCOUNT\_ID):

```json
{
 "Version":"2012-10-17",
 "Statement":[
  {
   "Effect":"Allow",
   "Action":["s3:GetObject","s3:ListBucket"],
   "Resource":["arn:aws:s3:::BUCKET_NAME","arn:aws:s3:::BUCKET_NAME/*"]
  },
  {
   "Effect":"Allow",
   "Action":["rekognition:DetectModerationLabels","rekognition:DetectLabels"],
   "Resource":"*"
  },
  {
   "Effect":"Allow",
   "Action":["dynamodb:PutItem","dynamodb:GetItem","dynamodb:UpdateItem"],
   "Resource":"arn:aws:dynamodb:REGION:ACCOUNT_ID:table/Images"
  },
  {
   "Effect":"Allow",
   "Action":"sns:Publish",
   "Resource":"arn:aws:sns:REGION:ACCOUNT_ID:image-moderation-alerts"
  },
  {
   "Effect":"Allow",
   "Action":["logs:CreateLogGroup","logs:CreateLogStream","logs:PutLogEvents"],
   "Resource":"arn:aws:logs:*:*:*"
  }
 ]
}
```

Attach it:

```bash
aws iam put-role-policy --role-name lambda-image-processor-role --policy-name lambda-minimal-policy --policy-document file://policy.json
```

---

# 5) Lambda code — processor (ready-to-deploy)

Create `lambda_processor/app.py` with this code (improved, logs, env vars, small safety):

```python
import os, json, uuid, logging
from datetime import datetime
import boto3

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

s3 = boto3.client('s3')
rek = boto3.client('rekognition')
ddb = boto3.resource('dynamodb')
sns = boto3.client('sns')

DDB_TABLE = os.environ.get('DDB_TABLE', 'Images')
SNS_ARN = os.environ.get('SNS_ARN')
FLAG_THRESHOLD = float(os.environ.get('FLAG_THRESHOLD', '60.0'))  # percent
TABLE = ddb.Table(DDB_TABLE)

def process_record(record):
    bucket = record['s3']['bucket']['name']
    key = record['s3']['object']['key']
    image_id = str(uuid.uuid4())
    s3_url = f"s3://{bucket}/{key}"
    upload_time = datetime.utcnow().isoformat()
    logger.info("Processing %s", s3_url)

    # Rekognition moderation
    moderation = rek.detect_moderation_labels(Image={'S3Object':{'Bucket':bucket,'Name':key}})
    mod_labels = moderation.get('ModerationLabels', [])
    flagged = any(l['Confidence'] >= FLAG_THRESHOLD for l in mod_labels)

    # Rekognition labels for tagging
    lbls_resp = rek.detect_labels(Image={'S3Object':{'Bucket':bucket,'Name':key}}, MaxLabels=10, MinConfidence=60)
    tags = [l['Name'] for l in lbls_resp.get('Labels', [])]

    item = {
        'image_id': image_id,
        's3_url': s3_url,
        'upload_time': upload_time,
        'tags': tags,
        'moderation': {
            'flagged': flagged,
            'labels': [{'Name': l['Name'], 'Confidence': l['Confidence']} for l in mod_labels]
        }
    }

    TABLE.put_item(Item=item)
    logger.info("Stored metadata for %s", image_id)

    if flagged and SNS_ARN:
        sns.publish(TopicArn=SNS_ARN, Subject='Image flagged', Message=json.dumps(item))
        logger.warning("Flagged image published to SNS: %s", image_id)

def lambda_handler(event, context):
    logger.info("Event: %s", event)
    try:
        for record in event.get('Records', []):
            process_record(record)
        return {"status":"ok"}
    except Exception as e:
        logger.exception("Unhandled exception")
        raise
```

Package and create zip:

```bash
cd lambda_processor
zip -r function.zip app.py
# find role ARN from earlier creation
aws lambda create-function --function-name image-moderation-processor \
  --zip-file fileb://function.zip --handler app.lambda_handler --runtime python3.11 \
  --role arn:aws:iam::ACCOUNT_ID:role/lambda-image-processor-role \
  --timeout 30 --memory-size 512
```

Set environment variables:

```bash
aws lambda update-function-configuration --function-name image-moderation-processor \
  --environment Variables="{DDB_TABLE=Images,SNS_ARN=$TOPIC_ARN,FLAG_THRESHOLD=60.0}"
```

---

# 6) Allow S3 to invoke Lambda and attach event (important)

Add permission so S3 can invoke the function:

```bash
aws lambda add-permission --function-name image-moderation-processor \
  --statement-id s3invoke --action "lambda:InvokeFunction" \
  --principal s3.amazonaws.com --source-arn arn:aws:s3:::${BUCKET_NAME}
```

Put bucket notification:
Create `notification.json`:

```json
{
  "LambdaFunctionConfigurations": [
    {
      "Id": "ImageUploadTrigger",
      "LambdaFunctionArn": "arn:aws:lambda:REGION:ACCOUNT_ID:function:image-moderation-processor",
      "Events": ["s3:ObjectCreated:Put"],
      "Filter": {
        "Key": {
          "FilterRules": [{"Name":"suffix","Value":".jpg"}, {"Name":"suffix","Value":".png"}]
        }
      }
    }
  ]
}
```

Then:

```bash
aws s3api put-bucket-notification-configuration --bucket $BUCKET_NAME --notification-configuration file://notification.json
```

---

# 7) Test it

* Upload a test image:

```bash
aws s3 cp sample.jpg s3://$BUCKET_NAME/test/sample.jpg
```

* Check CloudWatch logs for `image-moderation-processor` to see execution & Rekognition results.
* Check DynamoDB `Images` table for new item.
* If flagged, check admin email for SNS message (confirm subscription first).

---

# 8) Optional: Presigned URLs + simple uploader (recommended for frontend)

Make a small Lambda (or same service) that generates presigned POST or PUT so clients upload directly to S3 without exposing credentials.

Example presign snippet (python):

```python
s3_client = boto3.client('s3')
def generate_presigned_put(bucket, key, expires=300):
    return s3_client.generate_presigned_url('put_object', Params={'Bucket':bucket,'Key':key,'ACL':'private'}, ExpiresIn=expires)
```

Frontend: simple HTML + JS to `fetch` that presigned URL and PUT the file.

---

# 9) Search API (quick approach)

* Add Lambda + API Gateway endpoint `/search?tag=shirt`
* Implementation (small): DynamoDB `scan` with `FilterExpression=contains(tags, :t)` for small dataset. For production / large dataset use OpenSearch or model tags as GSI.

Example filter (boto3):

```python
from boto3.dynamodb.conditions import Attr
resp = table.scan(FilterExpression=Attr('tags').contains('shirt'))
```

---

# 10) CI/CD — quick GitHub Actions example (deploy via SAM or aws cloudformation)

`.github/workflows/deploy.yml` (sketch):

```yaml
name: Deploy
on:
  push:
    branches: [ main ]
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Configure AWS creds
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1
      - name: Deploy SAM
        run: |
          sam build
          sam deploy --stack-name image-moderation --capabilities CAPABILITY_IAM --no-confirm-changeset
```

(You must add deployment credentials as GitHub secrets.)

---

# 11) Monitoring, logging & alerts

* CloudWatch Logs: set retention (30 days) and create Metric Filter for `ERROR` or `Exception`.
* Create CloudWatch Alarm: Lambda errors > 1 within 5 minutes → SNS topic (same admin topic).
* Add dashboards: counts of processed, flagged ratio (use custom metrics or CloudWatch metric math).

---

# 12) Security & Production hardening (must-have)

* Block public access on S3. Use pre-signed uploads.
* Enable SSE (S3 server-side encryption) — use KMS for extra security.
* Least privilege IAM roles (narrow bucket ARNs).
* Use VPC endpoints for S3/API calls if needed.
* Use KMS to encrypt env vars or use Secrets Manager for secrets.
* Throttle uploads per user with API Gateway + Cognito (if required).
* Audit: enable CloudTrail.

---

# 13) Cost control & scaling tips

* Rekognition = cost per image. Monitor monthly usage and set budgets.
* Use SQS between S3 and Lambda if you expect bursts (buffering).
* DynamoDB: start with On-Demand (PAY\_PER\_REQUEST), then switch to provisioned with autoscaling if predictable.
* Consider batch processing: group images and call Rekognition in batch if allowed by API.

---

# 14) Repo structure & README checklist

Repo skeleton:

```
image-moderation/
 ├─ infra/           # SAM/CloudFormation or CDK
 ├─ src/
 │  ├─ processor/    # Lambda code (app.py, requirements.txt)
 │  ├─ presign/
 │  ├─ search/
 ├─ tests/
 ├─ .github/workflows/
 ├─ README.md
```

README should include:

* Short project summary & problem solved
* Architecture diagram (upload a PNG)
* Setup steps (prereqs, aws configure)
* Deploy steps (sam deploy or CLI steps)
* How to test (sample images)
* Cost & security notes
* Screenshots of sample DynamoDB item and SNS email

LinkedIn post idea (short):

> Built a serverless **Smart Image Moderation & Auto-Tagging** pipeline on AWS (S3 → Lambda → Rekognition → DynamoDB → SNS). Automatically flags inappropriate uploads, auto-tags for search, and notifies admins — great for e-commerce & social platforms. Repo: <link>

---

# 15) Testing plan (important)

* Unit tests: mock boto3 using `moto` or pytest-mock.
* Integration: deploy to dev account, upload test images for both flagged and non-flagged content.
* Edge cases: huge images, unsupported formats, Rekognition rate limits, partial failures (DynamoDB write fails).
* Load test: generate many simultaneous uploads; if throttled, add SQS.

---

# 16) Next actions — what do you want me to do now?

Pick one and I’ll **deliver it right here**:

1. Create a full **AWS SAM template** + deploy instructions (CloudFormation-style infra).
2. Produce a full **Lambda processor repo** (code + unit tests + requirements).
3. Build **presigned upload Lambda + HTML uploader** (complete small frontend).
4. Create **GitHub repo scaffold** and `.github/workflows/deploy.yml`.
5. Generate a polished **README + LinkedIn post + architecture diagram (ASCII or text)**.

Bata bhai — kaunsa step pe start karun abhi? Main uss item ka full code/template aur exact commands turant de dunga.
